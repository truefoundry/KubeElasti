{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Elasti is a Kubernetes-native solution that offers scale-to-zero functionality when there is no traffic and automatic scale up to 0 when traffic arrives. Most Kubernetes autoscaling solutions like HPA or Keda can scale from 1 to n replicas based on cpu utilization or memory usage. However, these solutions do not offer a way to scale to 0 when there is no traffic. Elasti solves this problem by dynamically managing service replicas based on real-time traffic conditions. It only handles scaling the application down to 0 replicas and scaling it back up to 1 replica when traffic is detected again. The scaling after 1 replica is handled by the autoscaler like HPA or Keda.</p> <p>The name Elasti comes from a superhero \"Elasti-Girl\" from DC Comics. Her superpower is to expand or shrink her body at will\u2014from hundreds of feet tall to mere inches in height.</p> <p>Elasti uses a proxy mechanism that queues and holds requests for scaled-down services, bringing them up only when needed. The proxy is used only when the service is scaled down to 0. When the service is scaled up to 1, the proxy is disabled and the requests are processed directly by the pods of the service.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<p>Elasti continuously monitors an ElastiService by evaluating a set of custom triggers defined in its configuration. These triggers represent various conditions\u2014such as traffic metrics or other custom signals\u2014that determine whether a service should be active or scaled down.</p> <ul> <li> <p>Scaling Down:   When all triggers indicate inactivity or low demand, Elasti scales the target service down to 0 replicas. During this period, elasti switches into proxy mode and queues incoming requests instead of dropping them.</p> </li> <li> <p>Traffic Queueing in Proxy Mode:   In Proxy Mode, Elasti intercepts and queues incoming requests directed at the scaled-down service. This ensures that no request is lost, even when the service is scaled down to 0.</p> </li> <li> <p>Scaling Up:   If any trigger signals a need for activity, Elasti immediately scales the service back up to its minimum replicas. As the service comes online, Elasti switches to Serve Mode.</p> </li> <li> <p>Serve Mode:   In Serve Mode, the active service handles all incoming traffic directly. Meanwhile, any queued requests accumulated during Proxy Mode are processed, ensuring a seamless return to full operational capacity.</p> </li> </ul> <p>This allows Elasti to optimize resource consumption by scaling services down when unneeded, while its request queueing mechanism preserves user interactions and guarantees prompt service availability when conditions change.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>Seamless Integration: Elasti integrates effortlessly with your existing Kubernetes setup - whether you are using HPA or Keda. It takes just a few steps to enable scale to zero for any service.</p> </li> <li> <p>Deployment and Argo Rollouts Support: Elasti supports two scale target references: Deployment and Argo Rollouts, making it versatile for various deployment scenarios.</p> </li> <li> <p>Prometheus Metrics Export: Elasti exports Prometheus metrics for easy out-of-the-box monitoring. You can also import a pre-built dashboard into Grafana for comprehensive visualization.</p> </li> <li> <p>Generic Service Support: Elasti works at the kubernetes service level. It also supports East-West traffic using cluster-local service DNS, ensuring robust and flexible traffic management across your services. So any ingress or service mesh solution can be used with Elasti.</p> </li> </ul>"},{"location":"#limitations","title":"Limitations","text":"<ul> <li>Only HTTP is supported: Elasti currently supports requests that are routed to the service via HTTP. In the future we will support more protocols like TCP, UDP etc.</li> <li>Only Deployment and Argo Rollouts are supported: Elasti supports two scale target references: Deployment and Argo Rollouts. In the future this will be made generic to support all target references that support the <code>/scale</code> subresource.</li> <li>Prometheus Trigger: The only trigger currently supported is Prometheus</li> </ul> <p>Please checkout the comparison here to see how Elasti compares to other Kubernetes autoscaling solutions.</p>"},{"location":"architecture/","title":"Elasti Architecture","text":"<p>Elasti comprises of two main components: operator and resolver.</p> <ul> <li>Controller: A Kubernetes controller built using kubebuilder. It monitors ElastiService resources and scaled them to 0 or 1 as needed.</li> <li>Resolver: A service that intercepts incoming requests for scaled-down services, queues them, and notifies the elasti-controller to scale up the target service.</li> </ul>"},{"location":"architecture/#flow-description","title":"Flow Description","text":"<p>When we enable Elasti on a service, the service operates in 3 modes:</p> <ol> <li>Steady State: The service is receiving traffic and doesn't need to be scaled down to 0.</li> <li>Scale Down to 0: The service hasn't received any traffic for the configured duration and can be scaled down to 0.</li> <li>Scale up from 0: The service receives traffic again and can be scaled up to the configured minTargetReplicas.</li> </ol> <p></p>"},{"location":"architecture/#steady-state-flow-of-requests-to-service","title":"Steady state flow of requests to service","text":"<p>In this mode, all the requests are handled directly by the service pods. The Elasti resolved doesn't come into the picture. Elasti controller keeps polling prometheus with the configured query and check the result with threshold value to see if the service can be scaled down.</p>"},{"location":"architecture/#scale-down-to-0-when-there-are-no-requests","title":"Scale down to 0 when there are no requests","text":"<p>If the query from prometheus returns a value less than the threshold, Elasti will scale down the service to 0. Before it scales to 0, it redirects the requests to be forwarded to the Elasti resolver and then modified the Rollout/deployment to have 0 replicas. It also then pauses Keda (if Keda is being used) to prevent it from scaling the service up since Keda is configured with minReplicas as 1. </p>"},{"location":"architecture/#scale-up-from-0-when-the-first-request-arrives","title":"Scale up from 0 when the first request arrives.","text":"<p>Since the service is scaled down to 0, all requests will hit the Elasti resolver. When the first request arrives, Elasti will scale up the service to the configured minTargetReplicas. It then resumes Keda to continue autoscaling in case there is a sudden burst of requests. It also changes the service to point to the actual service pods once the pod is up. The requests which came to ElastiResolver are retried till 5 mins and the response is sent back to the client. If the pod takes more than 5 mins to come up, the request is dropped.</p>"},{"location":"comparisons/","title":"Comparisons with Other Solutions","text":"<p>This document compares Elasti with other popular serverless and scale-to-zero solutions in the Kubernetes ecosystem.</p>"},{"location":"comparisons/#knative","title":"Knative","text":""},{"location":"comparisons/#overview","title":"Overview","text":"<p>Knative is a comprehensive platform for deploying and managing serverless workloads on Kubernetes. It provides a complete serverless experience with features like scale-to-zero, request-based autoscaling, and traffic management.</p>"},{"location":"comparisons/#key-differences","title":"Key Differences","text":"<ul> <li>Complexity: Knative is a full-featured platform that requires significant setup and maintenance. Elasti is focused solely on scale-to-zero functionality and can be added to existing services with minimal configuration.</li> <li>Integration: Knative requires services to be deployed as Knative services. Elasti works with existing Kubernetes deployments and Argo Rollouts without modification.</li> <li>Learning Curve: Knative has a steeper learning curve due to its many concepts and components. Elasti follows familiar Kubernetes patterns with simple CRD-based configuration.</li> </ul>"},{"location":"comparisons/#openfaas","title":"OpenFaaS","text":""},{"location":"comparisons/#overview_1","title":"Overview","text":"<p>OpenFaaS is a framework for building serverless functions with Docker and Kubernetes, making it easy to deploy serverless functions to any cloud or on-premises.</p>"},{"location":"comparisons/#key-differences_1","title":"Key Differences","text":"<ul> <li>Purpose: OpenFaaS is primarily designed for Function-as-a-Service (FaaS) workloads. Elasti is built for existing HTTP services.</li> <li>Architecture: OpenFaaS requires functions to be written and packaged in a specific way. Elasti works with any HTTP service without code changes.</li> <li>Scaling: OpenFaaS uses its own scaling mechanisms. Elasti integrates with existing autoscalers (HPA/KEDA) while adding scale-to-zero capability.</li> </ul>"},{"location":"comparisons/#keda-http-add-on","title":"KEDA HTTP Add-on","text":""},{"location":"comparisons/#overview_2","title":"Overview","text":"<p>KEDA HTTP Add-on is an extension to KEDA that enables HTTP-based scaling, including scale-to-zero functionality.</p>"},{"location":"comparisons/#key-differences_2","title":"Key Differences","text":"<ul> <li>Request Handling: </li> <li>KEDA http add-on inserts itself in the http path and handles requests even when the service has been scaled up.</li> <li>Elasti takes itself out of the http path once the service has been scaled up.</li> <li>Integration:</li> <li>KEDA HTTP Add-on requires KEDA installation and configuration.</li> <li>Elasti can work standalone or integrate with KEDA if needed.</li> </ul>"},{"location":"comparisons/#feature-comparison-table","title":"Feature Comparison Table","text":"Feature Elasti Knative OpenFaaS KEDA HTTP Add-on Scale to Zero \u2705 \u2705 \u2705 \u2705 Works with Existing Services \u2705 \u274c \u274c \u2705 Resource Footprint Low High Medium Low Setup Complexity Low High Medium Medium"},{"location":"comparisons/#when-to-choose-elasti","title":"When to Choose Elasti","text":"<p>Elasti is the best choice when you: 1. Need to add scale-to-zero capability to existing HTTP services 2. Want to ensure zero request loss during scaling operations 3. Prefer a lightweight solution with minimal configuration 4. Need integration with existing autoscalers (HPA/KEDA)</p>"},{"location":"configure-elastiservice/","title":"Configure ElastiService","text":"<p>To enable scale to 0 on any deployment, we will need to create an ElastiService custom resource for that deployment. </p> <p>A ElastiService custom resource has the following structure:</p> <pre><code>apiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: &lt;service-name&gt;\n  namespace: &lt;service-namespace&gt;\nspec:\n  minTargetReplicas: &lt;min-target-replicas&gt;\n  service: &lt;service-name&gt;\n  cooldownPeriod: &lt;cooldown-period&gt;\n  scaleTargetRef:\n    apiVersion: &lt;apiVersion&gt;\n    kind: &lt;kind&gt;\n    name: &lt;deployment-or-rollout-name&gt;\n  triggers:\n  - type: &lt;trigger-type&gt;\n    metadata:\n      &lt;trigger-metadata&gt;\n  autoscaler:\n    name: &lt;autoscaler-object-name&gt;\n    type: &lt;autoscaler-type&gt;\n</code></pre> <p>The key fields to be specified in the spec are:</p> <ul> <li><code>&lt;service-name&gt;</code>: Replace it with the service you want managed by elasti.</li> <li><code>&lt;service-namespace&gt;</code>: Replace by namespace of the service.</li> <li><code>&lt;min-target-replicas&gt;</code>: Min replicas to bring up when first request arrives.</li> <li><code>&lt;scaleTargetRef&gt;</code>: Reference to the scale target similar to the one used in HorizontalPodAutoscaler.</li> <li><code>&lt;kind&gt;</code>: Replace by <code>rollouts</code> or <code>deployments</code></li> <li><code>&lt;apiVersion&gt;</code>: Replace with <code>argoproj.io/v1alpha1</code> or <code>apps/v1</code></li> <li><code>&lt;deployment-or-rollout-name&gt;</code>: Replace with name of the rollout or the deployment for the service. This will be scaled up to min-target-replicas when first request comes</li> <li><code>cooldownPeriod</code>: Minimum time (in seconds) to wait after scaling up before considering scale down</li> <li><code>triggers</code>: List of conditions that determine when to scale down (currently supports only Prometheus metrics)</li> <li><code>autoscaler</code>: Optional integration with an external autoscaler (HPA/KEDA) if needed</li> <li><code>&lt;autoscaler-type&gt;</code>: keda</li> <li><code>&lt;autoscaler-object-name&gt;</code>: Name of the KEDA ScaledObject</li> </ul>"},{"location":"configure-elastiservice/#configuration-explanation","title":"Configuration Explanation","text":"<p>The section below explains how are the different configuration options used in Elasti.</p>"},{"location":"configure-elastiservice/#which-service-to-apply-elasti-on","title":"Which service to apply elasti on","text":"<p>This is defined using the <code>scaleTargetRef</code> field in the spec. </p> <ul> <li><code>scaleTargetRef.kind</code>: should be either be  <code>deployments</code> or <code>rollouts</code> (in case you are using Argo Rollouts). </li> <li><code>scaleTargetRef.apiVersion</code> will be <code>apps/v1</code> if you are using deployments or <code>argoproj.io/v1alpha1</code> in case you are using argo-rollouts. </li> <li><code>scaleTargetRef.name</code> should exactly match the name of the deployment or rollout. </li> </ul>"},{"location":"configure-elastiservice/#when-to-scale-down-the-service-to-0","title":"When to scale down the service to 0","text":"<p>This is defined uing the triggers field in the spec. Currently, Elasti supports only one trigger type - <code>prometheus</code>. The metadata field of the trigger defines the trigger data. The <code>query</code> field is the prometheus query to use for the trigger. The <code>serverAddress</code> field is the address of the prometheus server. The <code>threshold</code> field is the threshold value to use for the trigger. So we can define a query to check for the number of requests per second and the threshold to be 0. Elasti will check this metric every 30 seconds and if the values is less than 0(<code>threshold</code>) it will scale down the service to 0.</p> <p>An example trigger is as follows:</p> <pre><code>triggers:\n- type: prometheus\n    metadata:\n    query: sum(rate(nginx_ingress_controller_nginx_process_requests_total[1m])) or vector(0)\n    serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090\n    threshold: 0.5\n</code></pre> <p>Once the service is scaled down to 0, we also need to pause the current autoscaler to make sure it doesn't scale up the service again. While this is not a problem with HPA, Keda will scale up the service again since the min replicas is 1. Hence Elasti needs to know about the Keda scaled object so that it can pause it. This information is provided in the <code>autoscaler</code> field of the ElastiService. The autoscaler type supported as of now is only keda. </p> <ul> <li>autoscaler.name: Name of the keda scaled object</li> <li>autoscaler.type: keda</li> </ul>"},{"location":"configure-elastiservice/#when-to-scale-up-the-service-to-1","title":"When to scale up the service to 1","text":"<p>As soon as the service is scaled down to 0, Elasti resolved will start accepting requests for that service. On receiving the first request, it will scale up the service to <code>minTargetReplicas</code>. Once the pod is up, the new requests are handled by the service pods and do not pass through the elasti-resolver. The requests that came before the pod scaled up are held in memory of the elasti-resolver and are processed once the pod is up.</p> <p>We can configure the <code>cooldownPeriod</code> to specify the minimum time (in seconds) to wait after scaling up before considering scale down.</p>"},{"location":"development/","title":"Development Guide","text":"<p>Setting up your development environment for Elasti involves preparing your local setup for building, testing, and contributing to the project. Follow these steps to get started:</p>"},{"location":"development/#dev-environment","title":"Dev Environment","text":""},{"location":"development/#1-get-required-tools","title":"1. Get required tools","text":"<p>Ensure you have the following tools installed:</p> <ul> <li>Go: The programming language used for Elasti. Download and install it from golang.org.</li> <li>Docker: For containerization and building Docker images. Install it from docker.com.</li> <li>kubectl:: Command-line tool for interacting with Kubernetes. Install it from kubernetes.io.</li> <li>Helm: Package manager for Kubernetes. Install it from helm.sh.</li> <li>Docker Desktop/Kind/Minikube: A local kubernetes cluster. Make sure you have the local cluster running before development.</li> <li>Make: Helps in working with the project.</li> <li>Istio: Required to test the project with istio. Install from istio.io</li> <li>k6: Required to load test the project. Install from k6.io</li> </ul>"},{"location":"development/#2-clone-the-repository","title":"2. Clone the Repository","text":"<p>Clone the Elasti repository from GitHub to your local machine:</p> <pre><code>git clone https://github.com/truefoundry/elasti.git\ncd elasti\n</code></pre> <p>Make sure you checkout the documentation and architecture before making your changes.</p>"},{"location":"development/#3-repository-structure","title":"3. Repository Structure","text":"<p>Understanding the repository structure will help you navigate and contribute effectively to the Elasti project. Below is an overview of the key directories and files in the repository:</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 go.work\n\u251c\u2500\u2500 go.work.sum\n\u251c\u2500\u2500 kustomization.yaml\n\u251c\u2500\u2500 operator\n\u251c\u2500\u2500 pkg\n\u251c\u2500\u2500 playground\n\u251c\u2500\u2500 resolver\n\u2514\u2500\u2500 test\n</code></pre> <p>2 Main Modules:</p> <ul> <li><code>./operator</code>: Contains the code for Kubernetes operator, created using kubebuilder.   <code>.   \u251c\u2500\u2500 Dockerfile   \u251c\u2500\u2500 Makefile   \u251c\u2500\u2500 api   \u251c\u2500\u2500 cmd   \u251c\u2500\u2500 config   \u251c\u2500\u2500 go.mod   \u251c\u2500\u2500 go.sum   \u251c\u2500\u2500 internal   \u2514\u2500\u2500 test</code></li> <li><code>./api</code>: Contains the folder named after the apiVersion, and has custom resource type description.</li> <li><code>./config</code>: Kubernetes manifest files.</li> <li><code>./cmd</code>: Main files for the tool.</li> <li><code>./internal</code>: Internal packages of the program.</li> <li><code>./Makefile</code>: Helps with working with the program. Use <code>make help</code> to see all the available commands.</li> <li><code>./resolver</code>: Contains the code for resolver.</li> <li>File structure of it is similar to that of Operator.</li> </ul> <p>Other Directories:</p> <ul> <li><code>./playground</code>: Code to setup a playground to try and test elasti.</li> <li><code>./test</code>: Load testing scripts.</li> <li><code>./pkg</code>: Common packages, shared via Operator and Resolve.</li> <li><code>./charts</code>: Helm chart template.</li> <li><code>./docs</code>: Detailed documentation on the HLD, LLD and Architecture of elasti.</li> </ul>"},{"location":"development/#setup-playground","title":"Setup Playground","text":""},{"location":"development/#1-local-cluster","title":"1. Local Cluster","text":"<p>If you don't already have a local Kubernetes cluster, you can set one up using Minikube, Kind or Docker-Desktop:</p> <pre><code>minikube start\n</code></pre> <p>or</p> <pre><code>kind create cluster\n</code></pre> <p>or</p> <p>Enable it in Docker-Desktop</p>"},{"location":"development/#2-start-a-local-docker-registry","title":"2. Start a Local Docker Registry","text":"<p>Run a local Docker registry container, to push our images locally and access them in our cluster.</p> <pre><code>docker run -d -p 5001:5000 --name registry registry:2\n</code></pre> <p>You will need to add this registry to Minikube and Kind. With Docker-Desktop, it is automatically picked up if running in same context.</p> <p>Note: In MacOS, 5000 is not available, so we use 5001 instead.</p> <p>&lt;!-- ### 3. Install NGINX Ingress Controller: Install the NGINX Ingress Controller using Helm:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nkubectl create namespace nginx\nhelm install ingress-nginx ingress-nginx/ingress-nginx -n nginx\n``` --&gt;\n\n### 3. [Optional] Install Istio Gateway to work with istio\n\n```bash\n# Download the latest Istio release from the official Istio website.\ncurl -L https://istio.io/downloadIstio | sh -\n# Move it to home directory\nmv istio-x.xx.x ~/.istioctl\nexport PATH=$HOME/.istioctl/bin:$PATH\n\nistioctl install --set profile=default -y\n\n# Label the namespace where you want to deploy your application to enable Istio sidecar Injection\nkubectl create namespace demo\nkubectl label namespace demo istio-injection=enabled\n\n# Create a gateway\nkubectl apply -f ./playground/config/gateway.yaml\n</code></pre>"},{"location":"development/#4-deploy-a-demo-service","title":"4. Deploy a demo service","text":"<p>Run a demo application in your cluster.</p> <pre><code>kubectl create namespace demo\nkubectl apply -f ./playground/config/demo-application.yaml -n demo\n\n# Create a Virtual Service to expose the demo service if you are using istio\nkubectl apply -f ./playground/config/demo-virtualService.yaml -n demo\n</code></pre>"},{"location":"development/#5-build-publish-resolver","title":"5. Build &amp; Publish Resolver","text":"<p>Go into the resolver directory and run the build and publish command.</p> <pre><code>cd resolver\nmake docker-build docker-push IMG=localhost:5001/elasti-resolver:v1alpha1\n</code></pre>"},{"location":"development/#6-build-publish-operator","title":"6. Build &amp; Publish Operator","text":"<p>Go into the operator directory and run the build and publish command.</p> <pre><code>cd operator\nmake docker-build docker-push IMG=localhost:5001/elasti-operator:v1alpha1\n</code></pre>"},{"location":"development/#7-deploy-locally","title":"7. Deploy Locally","text":"<p>Make sure you have configured the local context in kubectl. We will be using <code>./playground/infra/elasti-demo-values.yaml</code> for the helm installation. Configure the image uri according to the requirement. Post that follow below steps from the project home directory:</p> <pre><code>kubectl create namespace elasti\nhelm template elasti ./charts/elasti -n elasti -f ./playground/infra/elasti-demo-values.yaml | kubectl apply -f -\n</code></pre> <p>If you want to enable monitoring, please make <code>enableMonitoring</code> true in the values file.</p>"},{"location":"development/#8-create-elastiservice-resource","title":"8. Create ElastiService Resource","text":"<p>Using the ElastiService Defination, create a manifest file for your service and apply it. For demo, we use the below manifest.</p> <pre><code>kubectl -n demo apply -f ./playground/config/demo-elastiService.yaml\n</code></pre>"},{"location":"development/#9-test-the-service","title":"9. Test the service","text":""},{"location":"development/#91-create-a-watch-on-the-service","title":"9.1 Create a watch on the service","text":"<pre><code>kubectl -n demo get elastiservice httpbin -w\n</code></pre>"},{"location":"development/#92-scale-down-the-service","title":"9.2 Scale down the service","text":"<pre><code>kubectl -n demo scale deployment httpbin --replicas=0\n</code></pre>"},{"location":"development/#93-create-a-load-on-the-service","title":"9.3 Create a load on the service","text":"<pre><code>kubectl run -it --rm curl --image=alpine/curl -- http://httpbin.demo.svc.cluster.local/headers\n</code></pre> <p>You should see the target service pod getting scaled up and response from the new pod.</p>"},{"location":"development/#testing","title":"Testing","text":"<p>This section outlines how to run integration tests, and performance tests using k6.</p> <ol> <li>Update k6 tests</li> </ol> <p>Update the <code>./test/load.js</code> file, to add your url for testing, and update other configurations in the same file.</p> <ol> <li>Run load.js</li> </ol> <p>Run the following command to run the test.</p> <p><code>chmod +x ./test/generate_load.sh    cd ./test    ./generate_load.sh</code></p> <ol> <li>Run E2E tests</li> </ol> <p>Use the KUTTL framework to execute Elasti's end-to-end tests in a real Kubernetes environment:</p> <pre><code>cd ./tests/e2e\nmake setup   # Sets up environment\nmake test    # Runs tests\n</code></pre> <p>For detailed information about the E2E test framework, see tests/e2e/README.md.</p>"},{"location":"development/#monitoring","title":"Monitoring","text":"<pre><code># First, add the prometheus-community Helm repository.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n\n# Install the kube-prometheus-stack chart. This chart includes Prometheus and Grafana.\nkubectl create namespace prometheus\nhelm install prometheus-stack prometheus-community/kube-prometheus-stack -n prometheus\n\n# Port-forward to access the dashboard\nkubectl port-forward -n prometheus services/prometheus-stack-grafana 3000:80\n\n# Get the admin user.\nkubectl get secret --namespace prometheus prometheus-stack-grafana -o jsonpath=\"{.data.admin-user}\" | base64 --decode ; echo\n# Get the admin password.\nkubectl get secret --namespace prometheus prometheus-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> <p>Post this, you can use <code>./playground/infra/elasti-dashboard.yaml</code> to import the elasti dashboard.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get started by following below steps:</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes Cluster: You should have a running Kubernetes cluster. You can use any cloud-based or on-premises Kubernetes distribution.</li> <li>kubectl: Installed and configured to interact with your Kubernetes cluster.</li> <li>Helm: Installed for managing Kubernetes applications.</li> </ul>"},{"location":"getting-started/#install","title":"Install","text":""},{"location":"getting-started/#1-install-elasti-using-helm","title":"1. Install Elasti using helm","text":"<p>Use Helm to install elasti into your Kubernetes cluster. </p> <pre><code>helm install elasti oci://tfy.jfrog.io/tfy-helm/elasti --namespace elasti --create-namespace\n</code></pre> <p>Check out values.yaml to see configuration options in the helm value file.</p>"},{"location":"getting-started/#2-verify-the-installation","title":"2. Verify the Installation","text":"<p>Check the status of your Helm release and ensure that the elasti components are running:</p> <pre><code>helm status elasti --namespace elasti\nkubectl get pods -n elasti\n</code></pre> <p>You will see 2 components running.</p> <ol> <li>Controller/Operator: <code>elasti-operator-controller-manager-...</code> is to switch the traffic, watch resources, scale etc.</li> <li>Resolver: <code>elasti-resolver-...</code> is to proxy the requests.</li> </ol>"},{"location":"getting-started/#3-setup-prometheus","title":"3. Setup Prometheus","text":"<p>We will setup a sample prometheus to read metrics from the nginx ingress controller.</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --set alertmanager.enabled=false \\\n  --set grafana.enabled=false \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\n</code></pre>"},{"location":"getting-started/#4-setup-nginx-ingress-controller","title":"4. Setup nginx ingress controller","text":"<p>We will setup a nginx ingress controller to route the traffic to the httpbin service.</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --set controller.metrics.enabled=true \\\n  --set controller.metrics.serviceMonitor.enabled=true \\\n  --create-namespace\n</code></pre> <p>This will deploy a nginx ingress controller in the <code>ingress-nginx</code> namespace.</p>"},{"location":"getting-started/#5-setup-a-service","title":"5. Setup a Service","text":"<p>We will use a sample httpbin service to demonstrate how to configure a service to handle its traffic via elasti.</p> <pre><code>kubectl create namespace elasti-demo\nkubectl apply -n elasti-demo -f https://raw.githubusercontent.com/truefoundry/elasti/refs/heads/main/playground/config/demo-application.yaml\n</code></pre> <p>This will deploy a httpbin service in the <code>elasti-demo</code> namespace.</p>"},{"location":"getting-started/#6-define-an-elastiservice","title":"6. Define an ElastiService","text":"<p>To configure a service to handle its traffic via elasti, you'll need to create and apply a <code>ElastiService</code> custom resource:</p> <p>Create a file named <code>httpbin-elasti.yaml</code> and apply the configuration.</p> <pre><code>apiVersion: elasti.truefoundry.com/v1alpha1\nkind: ElastiService\nmetadata:\n  name: httpbin-elasti\n  namespace: elasti-demo\nspec:\n  minTargetReplicas: 1\n  service: httpbin\n  cooldownPeriod: 5\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: deployments\n    name: httpbin\n  triggers:\n    - type: prometheus\n      metadata:\n        query: sum(rate(nginx_ingress_controller_nginx_process_requests_total[1m])) or vector(0)\n        serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090\n        threshold: \"0.5\"\n</code></pre>"},{"location":"getting-started/#7-apply-the-elasti-service-configuration","title":"7. Apply the elasti service configuration","text":"<p>Apply the configuration to your Kubernetes cluster:</p> <pre><code>kubectl apply -f httpbin-elasti.yaml -n elasti-demo\n</code></pre> <p>The pod will be scaled down to 0 replicas if there is no traffic.</p>"},{"location":"getting-started/#8-test-the-setup","title":"8. Test the setup","text":"<p>You can test the setup by sending requests to the nginx load balancer service.</p> <pre><code>kubectl port-forward svc/nginx-ingress-nginx-controller -n ingress-nginx 8080:80\n</code></pre> <p>Start a watch on the httpbin service.</p> <pre><code>kubectl get pods -n elasti-demo -w\n</code></pre> <p>Send a request to the httpbin service.</p> <pre><code>curl -v http://localhost:8080/httpbin\n</code></pre> <p>You should see the pods being created and scaled up to 1 replica. A response from the httpbin service should be visible for the curl command. The service should be scaled down to 0 replicas if there is no traffic for 5 (<code>cooldownPeriod</code> in elastiService) seconds.</p>"},{"location":"getting-started/#uninstall","title":"Uninstall","text":"<p>To uninstall Elasti, you will need to remove all the installed ElastiServices first. Then, simply delete the installation file.</p> <pre><code>kubectl delete elastiservices --all\nhelm uninstall elasti -n elasti\nkubectl delete namespace elasti\n</code></pre>"},{"location":"integration-hpa/","title":"Integration with HPA","text":"<p>Elasti works seamlessly with Horizontal Pod Autoscaler (HPA) and handles scaling to zero on its own. Since Elasti manages the scale-to-zero functionality, you can configure HPA to handle scaling based on metrics for any number of replicas greater than zero, while Elasti takes care of scaling to/from zero.</p> <p>A setup is explained in the getting started guide.</p>"},{"location":"integration-keda/","title":"Integration with KEDA","text":"<p>Elasti takes care of scaling up and down a service when there is some traffic. KEDA is a good candidate for performing the scaling logic for the service from minReplicas to maxReplicas based on it's triggers.</p> <p>Here we will see how to integrate Elasti with KEDA to build a complete scaling solution.</p>"},{"location":"integration-keda/#prerequisites","title":"Prerequisites","text":"<ul> <li>Make sure you have gone through the getting started guide. We will extend the same setup for this integration.</li> <li>KEDA installed in the cluster - KEDA Installation</li> </ul>"},{"location":"integration-keda/#steps","title":"Steps","text":""},{"location":"integration-keda/#1-create-a-keda-scaler-for-the-service","title":"1. Create a keda scaler for the service","text":"<p>Let's create a keda scaler for the httpbin service.</p> <pre><code>kubectl apply -f ./playground/config/ demo-application-keda.yaml\n</code></pre> <p>Note that the same prometheus query is used as in the getting started guide for ElastiService and the namespace is the same as the namespace that the ElastiService is created in.</p> <p>Refer to the keda documentation for more details on configuring the ScaledObject.</p>"},{"location":"integration-keda/#2-update-elastiservice-to-work-with-the-keda-scaler","title":"2. Update ElastiService to work with the keda scaler","text":"<p>We will update the ElastiService to specify the keda scaler to work with. We will add the following fields to the ElastiService object:</p> <pre><code>spec:\n  autoscaler:\n    name: httpbin-scaled-object\n    type: keda\n</code></pre> <p>Patch the ElastiService object with the above changes.</p> <pre><code>kubectl patch elastiservice httpbin -n elasti-demo -p '{\"spec\":{\"autoscaler\":{\"name\": \"httpbin-scaled-object\", \"type\": \"keda\"}}}' --type=merge\n</code></pre> <p>Now when elasti scales down the service, it will pause the keda ScaledObject to prevent it from scaling up the service again, and when elasti scales up the service, it will resume the ScaledObject.</p> <p>With these changes, elasti can reliably scale up the service when there is traffic and scale down the service to zero when there is no traffic while keda can handle the scaling logic for the service from minReplicas to maxReplicas based on it's triggers.</p>"},{"location":"monitoring/","title":"Monitoring","text":""},{"location":"monitoring/#monitoring","title":"Monitoring","text":"<p>Set <code>.global.enableMonitoring</code> to <code>true</code> in the values.yaml file to enable monitoring.</p> <p>This will create two ServiceMonitor custom resources to enable Prometheus to discover the Elasti components. To verify this, you can open your Prometheus interface and search for metrics prefixed with <code>elasti_</code>, or navigate to the Targets section to check if Elasti is listed.</p> <p>Once verification is complete, you can use the provided Grafana dashboard to monitor the internal metrics and performance of Elasti.</p> Grafana dashboard"}]}